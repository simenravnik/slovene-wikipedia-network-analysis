{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv2D, Conv1D, MaxPooling1D, LeakyReLU\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/train_test_spit.json\", \"r\") as f:\n",
    "    split = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILENAME = os.path.join(\"..\", \"..\", \"data\", \"wikilinks_train.emb\")\n",
    "embeddings = np.genfromtxt(EMBEDDING_FILENAME, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(X, y, embeddings, links, class_):\n",
    "    for i, j in links:\n",
    "        X.append(np.concatenate((embeddings[i], embeddings[j]), axis=0))\n",
    "        y.append(class_)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_1 = random.sample(split[\"train\"][\"1\"], int(len(split[\"train\"][\"1\"]) * 0.02))\n",
    "tr_0 = random.sample(split[\"train\"][\"0\"], int(len(split[\"train\"][\"0\"]) * 0.02))\n",
    "\n",
    "te_1 = random.sample(split[\"test\"][\"1\"], int(len(split[\"test\"][\"1\"]) * 0.02))\n",
    "te_0 = random.sample(split[\"test\"][\"0\"], int(len(split[\"test\"][\"0\"]) * 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "X_train, y_train = construct_dataset(X_train, y_train, embeddings, tr_1, 1)\n",
    "X_train, y_train = construct_dataset(X_train, y_train, embeddings, tr_0, 0)\n",
    "\n",
    "X_test, y_test = construct_dataset(X_train, y_train, embeddings, te_1, 1)\n",
    "X_test, y_test = construct_dataset(X_train, y_train, embeddings, te_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data for the CNN architecture\n",
    "X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_cnn = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(X_train_cnn.shape)\n",
    "print(X_valid_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu', input_shape = (X_train_cnn.shape[1], 1)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# compile the model - use categorical crossentropy, and the adam optimizer\n",
    "model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_cnn, y_train, batch_size = 128, epochs=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_preds_hybrid = model.predict(X_valid_cnn, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_actual, y_pred):\n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, y_pred)\n",
    "    recall = recall_score(y_actual, y_pred)\n",
    "    precision = precision_score(y_actual, y_pred)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_preds_hybrid[y_valid_preds_hybrid < 0.5] = 0\n",
    "y_valid_preds_hybrid[y_valid_preds_hybrid >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Valid')\n",
    "print_report(y_test, y_valid_preds_hybrid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
